<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>GPU Implementation — ANN Documentation</title>
  <link rel="stylesheet" href="style.css">
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" async></script>
  <script>document.addEventListener('DOMContentLoaded',()=>{document.querySelector('.hamburger')?.addEventListener('click',()=>{document.querySelector('.sidebar').classList.toggle('open')})});</script>
</head>
<body>
<header class="site-header">
  <button class="hamburger" aria-label="Menu">☰</button>
  <h1>ANN</h1><span class="subtitle">Artificial Neural Network Library</span>
</header>
<div class="layout">
<aside class="sidebar"><nav>
  <div class="section-title">Getting Started</div>
  <a href="index.html">Home</a>
  <a href="quickstart.html">Quick Start &amp; Build</a>
  <a href="examples.html">Code Examples</a>
  <div class="section-title">Reference</div>
  <a href="api.html">API Reference</a>
  <a href="architecture.html">Architecture</a>
  <div class="section-title">Deep Dive</div>
  <a href="math.html">Mathematical Foundations</a>
  <a href="gpu.html" class="active">GPU Implementation</a>
</nav></aside>
<main class="main">

<h1>GPU Implementation (OpenCL)</h1>
<p>The ANN library accelerates training and inference on GPUs via OpenCL. This page covers the kernel architecture, buffer layout, and multi-GPU coordination.</p>

<div class="toc">
  <h4>On this page</h4>
  <ol>
    <li><a href="#overview">Architecture Overview</a></li>
    <li><a href="#kernels">Kernel Files</a></li>
    <li><a href="#buffers">Buffer Layout</a></li>
    <li><a href="#forward-kernels">Forward Pass Kernels</a></li>
    <li><a href="#backward-kernels">Backward Pass Kernels</a></li>
    <li><a href="#update-kernels">Update Kernels</a></li>
    <li><a href="#multigpu">Multi-GPU Coordination</a></li>
    <li><a href="#lazy">Lazy Kernel Compilation</a></li>
  </ol>
</div>

<h2 id="overview">1. Architecture Overview</h2>

<figure class="diagram">
<svg viewBox="0 0 660 260" xmlns="http://www.w3.org/2000/svg" font-family="system-ui,sans-serif" font-size="13">
  <!-- CoreGPU -->
  <rect x="220" y="10" width="220" height="44" rx="8" fill="#fef7e0" stroke="#f9ab00" stroke-width="2"/>
  <text x="330" y="28" text-anchor="middle" font-weight="700" fill="#b06000">CoreGPU&lt;T&gt;</text>
  <text x="330" y="44" text-anchor="middle" font-size="11" fill="#5f6368">Orchestrator</text>

  <!-- Lines to workers -->
  <line x1="260" y1="54" x2="110" y2="90" stroke="#dadce0" stroke-width="1.5"/>
  <line x1="330" y1="54" x2="330" y2="90" stroke="#dadce0" stroke-width="1.5"/>
  <line x1="400" y1="54" x2="550" y2="90" stroke="#dadce0" stroke-width="1.5"/>

  <!-- Worker 0 -->
  <rect x="20" y="90" width="180" height="80" rx="8" fill="#fce8e6" stroke="#c5221f" stroke-width="1.5"/>
  <text x="110" y="112" text-anchor="middle" font-weight="600" fill="#c5221f">Worker 0 (GPU 0)</text>
  <text x="110" y="130" text-anchor="middle" font-size="10" fill="#5f6368">OpenCL kernels</text>
  <text x="110" y="145" text-anchor="middle" font-size="10" fill="#5f6368">GPU buffers</text>
  <text x="110" y="160" text-anchor="middle" font-size="10" fill="#5f6368">Samples [0..k]</text>

  <!-- Worker 1 -->
  <rect x="240" y="90" width="180" height="80" rx="8" fill="#fce8e6" stroke="#c5221f" stroke-width="1.5"/>
  <text x="330" y="112" text-anchor="middle" font-weight="600" fill="#c5221f">Worker 1 (GPU 1)</text>
  <text x="330" y="130" text-anchor="middle" font-size="10" fill="#5f6368">OpenCL kernels</text>
  <text x="330" y="145" text-anchor="middle" font-size="10" fill="#5f6368">GPU buffers</text>
  <text x="330" y="160" text-anchor="middle" font-size="10" fill="#5f6368">Samples [k+1..2k]</text>

  <!-- Worker N -->
  <rect x="460" y="90" width="180" height="80" rx="8" fill="#fce8e6" stroke="#c5221f" stroke-width="1.5"/>
  <text x="550" y="112" text-anchor="middle" font-weight="600" fill="#c5221f">Worker N (GPU N)</text>
  <text x="550" y="130" text-anchor="middle" font-size="10" fill="#5f6368">OpenCL kernels</text>
  <text x="550" y="145" text-anchor="middle" font-size="10" fill="#5f6368">GPU buffers</text>
  <text x="550" y="160" text-anchor="middle" font-size="10" fill="#5f6368">Samples [..N·k]</text>

  <!-- Merge -->
  <line x1="110" y1="170" x2="330" y2="210" stroke="#34a853" stroke-width="1.5"/>
  <line x1="330" y1="170" x2="330" y2="210" stroke="#34a853" stroke-width="1.5"/>
  <line x1="550" y1="170" x2="330" y2="210" stroke="#34a853" stroke-width="1.5"/>

  <rect x="220" y="205" width="220" height="40" rx="8" fill="#e6f4ea" stroke="#34a853" stroke-width="2"/>
  <text x="330" y="230" text-anchor="middle" font-weight="600" fill="#137333">Merge Gradients &amp; Update</text>
</svg>
<figcaption>Figure 1 — Multi-GPU architecture. Each worker processes a subset of samples; gradients are merged after each epoch.</figcaption>
</figure>

<h2 id="kernels">2. Kernel Files</h2>
<p>Located in <code>opencl/</code>. The <code>CoreGPUWorker</code> loads them using <code>OpenCLWrapper::addSourceFile()</code>:</p>
<table>
  <tr><th>File</th><th>Contents</th></tr>
  <tr><td><code>Defines.hpp.cl</code></td><td>TYPE macro (<code>float</code>), <code>ActvFuncType</code> enum, <code>Layer</code> struct</td></tr>
  <tr><td><code>ActvFunc.cpp.cl</code></td><td>GPU activation functions &amp; derivatives (relu, sigmoid, tanh)</td></tr>
  <tr><td><code>IdxHelper.cpp.cl</code></td><td>Index calculation helpers for flattened buffers</td></tr>
  <tr><td><code>Kernels.cpp.cl</code></td><td>All computation kernels (forward, backward, accumulate, update)</td></tr>
</table>

<div class="warn-box">
  The <code>Layer</code> struct and <code>ActvFuncType</code> enum in the .cl files <strong>must match</strong> the C++ definitions exactly — same field order, same enum values.
</div>

<h2 id="buffers">3. Buffer Layout</h2>
<p>All tensors are <strong>flattened to 1-D arrays</strong> for GPU transfer. Index helpers compute offsets:</p>

<table>
  <tr><th>Buffer</th><th>Size Formula</th><th>Index Function</th></tr>
  <tr><td><code>actvs</code></td><td>\(\sum_{l=0}^{L} n_l\)</td><td><code>getActvIndex(l, j)</code></td></tr>
  <tr><td><code>zs</code></td><td>\(\sum_{l=0}^{L} n_l\)</td><td><code>getZIndex(l, j)</code></td></tr>
  <tr><td><code>weights</code></td><td>\(\sum_{l=1}^{L} n_l \cdot n_{l-1}\)</td><td><code>getWeightIndex(l, j, k)</code></td></tr>
  <tr><td><code>biases</code></td><td>\(\sum_{l=0}^{L} n_l\)</td><td><code>getBiasIndex(l, j)</code></td></tr>
</table>

<p>The accumulator buffers (<code>accum_dCost_dWeights</code>, <code>accum_dCost_dBiases</code>) have the same sizes as <code>weights</code> and <code>biases</code>, respectively.</p>

<h2 id="forward-kernels">4. Forward Pass Kernels</h2>
<p>For each layer \( l \), two kernels are enqueued sequentially:</p>

<div class="card">
<h3><code>calculate_zs</code></h3>
<p><strong>Work items:</strong> \( n_l \) (one per neuron in layer \( l \))</p>
<pre><code><span class="comment">// Each work-item j computes:</span>
z = biases[getBiasIndex(l, j)];
<span class="keyword">for</span> (k = <span class="number">0</span>; k &lt; n_{l-1}; k++)
    z += weights[getWeightIndex(l, j, k)] * actvs[getActvIndex(l-<span class="number">1</span>, k)];
zs[getZIndex(l, j)] = z;
</code></pre>
</div>

<div class="card">
<h3><code>calculate_actvs</code></h3>
<p><strong>Work items:</strong> \( n_l \)</p>
<pre><code><span class="comment">// Each work-item j computes:</span>
actvs[getActvIndex(l, j)] = actvFunc_calculate(zs[getZIndex(l, j)], layers[l].actvFuncType);
</code></pre>
</div>

<h2 id="backward-kernels">5. Backward Pass Kernels</h2>

<div class="card">
<h3><code>calculate_dCost_dActv_last_layer</code></h3>
<p><strong>Work items:</strong> \( n_L \) (output layer neurons)</p>
<pre><code>dCost_dActvs[j] = <span class="number">2.0</span> * (actvs[j] - outputs[j]);
</code></pre>
</div>

<div class="card">
<h3><code>calculate_dCost_dActv</code></h3>
<p><strong>Work items:</strong> \( n_l \) (hidden layer neurons). Computed <em>from the last hidden layer backwards</em>.</p>
<pre><code>sum = <span class="number">0</span>;
<span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; n_{l+1}; j++)
    sum += weights[l+<span class="number">1</span>][j][k] * derivative(z[l+<span class="number">1</span>][j]) * dCost_dActvs[l+<span class="number">1</span>][j];
dCost_dActvs[l][k] = sum;
</code></pre>
</div>

<div class="card">
<h3><code>calculate_dCost_dWeight</code> &amp; <code>calculate_dCost_dBias</code></h3>
<p><strong>Work items:</strong> \( n_l \times n_{l-1} \) for weights, \( n_l \) for biases.</p>
<pre><code><span class="comment">// Weight gradient:</span>
dWeight[l][j][k] = actvs[l-<span class="number">1</span>][k] * derivative(z[l][j]) * dCost_dActvs[l][j];

<span class="comment">// Bias gradient:</span>
dBias[l][j] = derivative(z[l][j]) * dCost_dActvs[l][j];
</code></pre>
</div>

<h2 id="update-kernels">6. Accumulate &amp; Update Kernels</h2>

<div class="card">
<h3><code>accumulate_dCost_dWeights</code> / <code>accumulate_dCost_dBiases</code></h3>
<p>Adds current sample gradients to running accumulators:</p>
<pre><code>accum[i] += gradient[i];
</code></pre>
</div>

<div class="card">
<h3><code>update_weights</code> / <code>update_biases</code></h3>
<p>SGD update with learning rate:</p>
<pre><code>param[i] -= learningRate * (accum[i] / numSamples);
</code></pre>
</div>

<h2 id="multigpu">7. Multi-GPU Coordination</h2>
<p>When multiple GPUs are available, <code>CoreGPU</code> distributes samples evenly across workers:</p>

<ol>
  <li><strong>Distribution:</strong> Each GPU processes <code>totalSamples / numGPUs</code> samples per epoch</li>
  <li><strong>Parallel execution:</strong> <code>QtConcurrent::blockingMap()</code> runs all GPU workers simultaneously</li>
  <li><strong>Gradient merging:</strong> After the epoch, <code>mergeGradients()</code> reads accumulators from each GPU, sums them, and writes the combined result back to all GPUs</li>
  <li><strong>Update:</strong> Each GPU applies the same update, keeping parameters synchronised</li>
</ol>

<pre><code><span class="comment">// Gradient merging pseudocode (CoreGPU::mergeGradients)</span>
totalWeightGrads = <span class="number">0</span>;  totalBiasGrads = <span class="number">0</span>;
<span class="keyword">for each</span> worker:
    worker.<span class="func">readAccumulatedGradients</span>(wGrads, bGrads);
    totalWeightGrads += wGrads;
    totalBiasGrads   += bGrads;

<span class="keyword">for each</span> worker:
    worker.<span class="func">setAccumulators</span>(totalWeightGrads, totalBiasGrads);
</code></pre>

<h2 id="lazy">8. Lazy Kernel Compilation</h2>
<p>Kernels are built the first time they're needed. <code>CoreGPUWorker</code> maintains flags:</p>
<ul>
  <li><code>predictKernelsSetup</code> — forward-only pipeline</li>
  <li><code>trainingKernelsSetup</code> — forward + backward + accumulate</li>
  <li><code>backpropagateKernelsSetup</code> — for step-by-step API</li>
  <li><code>accumulateKernelsSetup</code> — accumulate-only pipeline</li>
</ul>
<p>Each setup method calls <code>OpenCLWrapper::addSourceFile()</code> and <code>addKernel()</code>, then sets the flag to prevent recompilation.</p>

<div class="info-box">
  Kernel compilation can take a few seconds on the first call, but subsequent calls reuse the compiled kernel. This is especially important for training, where the forward/backward pipeline is reused for every sample.
</div>

</main>
</div>
<footer class="site-footer">ANN Library Documentation</footer>
</body></html>

